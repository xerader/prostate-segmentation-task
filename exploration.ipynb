{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the .nii file\n",
    "import nibabel as nib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# now read in the image\n",
    "segmentation = nib.load('./data/BIDMC/processed/Case02_segmentation.nii')\n",
    "img = nib.load('./data/BIDMC/processed/Case02.nii')\n",
    "# Compute the Z projection by summing along the Z-axis\n",
    "z_projection = img.get_fdata().sum(axis=2)\n",
    "seg_projection = segmentation.get_fdata().sum(axis=2)\n",
    "\n",
    "# Display the Z projection\n",
    "plt.imshow(z_projection, cmap='gray')\n",
    "plt.imshow(seg_projection, alpha=0.1, cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "# check that all packages are installed (see requirements.txt file)\n",
    "required_packages = {\n",
    "    \"jupyter\",\n",
    "    \"numpy\",\n",
    "    \"matplotlib\",\n",
    "    \"ipywidgets\",\n",
    "    \"scipy\",\n",
    "    \"pandas\",\n",
    "    \"numba\",\n",
    "    \"multiprocess\",\n",
    "    \"SimpleITK\",\n",
    "}\n",
    "\n",
    "problem_packages = list()\n",
    "# Iterate over the required packages: If the package is not installed\n",
    "# ignore the exception.\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        p = importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        problem_packages.append(package)\n",
    "\n",
    "if len(problem_packages) == 0:\n",
    "    print(\"All is well.\")\n",
    "else:\n",
    "    print(\n",
    "        \"The following packages are required but not installed: \"\n",
    "        + \", \".join(problem_packages)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        image = torch.from_numpy(image).float()\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "            \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "\n",
    "def resample_image(image, target_size=(256, 256), is_mask=False):\n",
    "    # Get current image size\n",
    "    original_size = image.GetSize()\n",
    "    original_spacing = image.GetSpacing()\n",
    "    \n",
    "    # Calculate new spacing to fit target size\n",
    "    new_size = [target_size[0], target_size[1], original_size[2]]\n",
    "    new_spacing = [\n",
    "        original_spacing[0] * original_size[0] / new_size[0],\n",
    "        original_spacing[1] * original_size[1] / new_size[1],\n",
    "        original_spacing[2]\n",
    "    ]\n",
    "    \n",
    "    # Set interpolator\n",
    "    interpolator = sitk.sitkNearestNeighbor if is_mask else sitk.sitkLinear\n",
    "    \n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetSize(new_size)\n",
    "    resample.SetOutputSpacing(new_spacing)\n",
    "    resample.SetOutputOrigin(image.GetOrigin())\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "    resample.SetInterpolator(interpolator)\n",
    "    \n",
    "    return resample.Execute(image)\n",
    "\n",
    "def normalize_image(img_slice):\n",
    "    # Normalize to [0,1], avoid divide by zero\n",
    "    min_val = np.min(img_slice)\n",
    "    max_val = np.max(img_slice)\n",
    "    if max_val - min_val > 1e-5:\n",
    "        return (img_slice - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        return np.zeros_like(img_slice)\n",
    "\n",
    "def load_and_preprocess_data(data_dir='./data', target_size=(256, 256)):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for folder in glob.glob(os.path.join(data_dir, '*')):\n",
    "        if not os.path.isdir(folder) or not os.path.exists(os.path.join(folder, 'processed')):\n",
    "            continue\n",
    "        img_files = glob.glob(os.path.join(folder, 'processed', 'Case*.nii'))\n",
    "        for img_file in img_files:\n",
    "            if '_segmentation' in img_file:\n",
    "                continue\n",
    "            case_id = os.path.basename(img_file).split('.')[0]\n",
    "            seg_file = os.path.join(folder, 'processed', f\"{case_id}_segmentation.nii\")\n",
    "            if os.path.exists(seg_file):\n",
    "                print(f\"Processing {img_file} and {seg_file}\")\n",
    "                # Read using SimpleITK\n",
    "                sitk_img = sitk.ReadImage(img_file)\n",
    "                sitk_seg = sitk.ReadImage(seg_file)\n",
    "                \n",
    "                # Optionally, resample both to target_size\n",
    "                if target_size is not None:\n",
    "                    sitk_img = resample_image(sitk_img, target_size=target_size, is_mask=False)\n",
    "                    sitk_seg = resample_image(sitk_seg, target_size=target_size, is_mask=True)\n",
    "                \n",
    "                img_arr = sitk.GetArrayFromImage(sitk_img)  # shape: [slices, H, W]\n",
    "                seg_arr = sitk.GetArrayFromImage(sitk_seg)\n",
    "                \n",
    "                # Iterate through slices\n",
    "                for i in range(img_arr.shape[0]):\n",
    "                    img_slice = img_arr[i]\n",
    "                    seg_slice = seg_arr[i]\n",
    "                    if np.max(seg_slice) > 0:\n",
    "                        img_slice = normalize_image(img_slice)\n",
    "                        seg_slice = (seg_slice > 0).astype(np.float32)  # Binarize if needed\n",
    "                        images.append(img_slice)\n",
    "                        masks.append(seg_slice)\n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images)\n",
    "    masks = np.array(masks)\n",
    "    # [N, H, W] -> [N, 1, H, W]\n",
    "    images = np.expand_dims(images, axis=1)\n",
    "    masks = np.expand_dims(masks, axis=1)\n",
    "    return images.astype(np.float32), masks.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "images, masks = load_and_preprocess_data()\n",
    "\n",
    "images = images  # Limit to first 100 images for testing\n",
    "masks = masks    # Limit to first 100 masks for testing\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = MedicalImageDataset(X_train, y_train)\n",
    "val_dataset = MedicalImageDataset(X_val, y_val)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder (Contracting Path)\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.enc4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bridge\n",
    "        self.bridge = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Decoder (Expanding Path)\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(1024, 512)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec4 = DoubleConv(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool1(enc1))\n",
    "        enc3 = self.enc3(self.pool2(enc2))\n",
    "        enc4 = self.enc4(self.pool3(enc3))\n",
    "        \n",
    "        # Bridge\n",
    "        bridge = self.bridge(self.pool4(enc4))\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        dec1 = self.dec1(torch.cat([self.up1(bridge), enc4], dim=1))\n",
    "        dec2 = self.dec2(torch.cat([self.up2(dec1), enc3], dim=1))\n",
    "        dec3 = self.dec3(torch.cat([self.up3(dec2), enc2], dim=1))\n",
    "        dec4 = self.dec4(torch.cat([self.up4(dec3), enc1], dim=1))\n",
    "        \n",
    "        # Output\n",
    "        out = torch.sigmoid(self.out(dec4))\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "model = UNet().to(device)\n",
    "print(model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs, masks)\n",
    "                \n",
    "                running_val_loss += val_loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), 'unet_model_best.pth')\n",
    "            print(f'Model saved at epoch {epoch+1}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 2\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use the model to predict on images and generate masks\n",
    "model.load_state_dict(torch.load('unet_model_best.pth'))\n",
    "\n",
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Predict on validation set\n",
    "predictions = predict(model, val_loader)\n",
    "# Convert predictions to binary masks\n",
    "predictions = (predictions > 0.5).astype(np.float32)\n",
    "# Visualize some predictions\n",
    "\n",
    "def visualize_predictions(images, masks, predictions, num_images=4):\n",
    "    plt.figure(figsize=(14, 3 * num_images))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        plt.subplot(num_images, 2, i * 3 + 1)\n",
    "        plt.imshow(images[i, -1], cmap='gray')\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_images, 2, i * 3 + 2)\n",
    "        plt.imshow(masks[i, -1], cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(num_images, 2, i * 3 + 3)\n",
    "        plt.imshow(predictions[i, -1], cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the .npy files \n",
    "import numpy as np\n",
    "\n",
    "unet_masks = np.load('./unet_masks.npy')\n",
    "unet_outputs = np.load('./unet_outputs.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(unet_outputs, unet_masks, predictions, num_images=5)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
